{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from cag.embeddings import SentenceTransformerEmbeddings\n",
    "from cag.models import ChatOllama\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:57:41.326477Z",
     "start_time": "2024-12-02T11:57:38.638509Z"
    }
   },
   "cell_type": "code",
   "source": "embeddings_model = SentenceTransformerEmbeddings('sentence-transformers/all-mpnet-base-v2', device='cuda')",
   "id": "4cd25110fb018f0b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T19:36:28.487581Z",
     "start_time": "2024-11-28T19:36:28.035494Z"
    }
   },
   "cell_type": "code",
   "source": "qwen = ChatOllama(model = 'qwen2.5', temprature = 0.001)",
   "id": "d0312c2443bcebdb",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T18:47:00.043007Z",
     "start_time": "2024-11-28T18:46:59.618567Z"
    }
   },
   "cell_type": "code",
   "source": "llama = ChatOllama(model = 'llama3.2', temprature = 0.001)",
   "id": "b266d7ec55751b33",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T19:36:31.725233Z",
     "start_time": "2024-11-28T19:36:31.708229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def answer_relevancy(generated_answer, original_query):\n",
    "    \n",
    "    prompt = \"\"\"i will give you a answer , please generate three question which we can derive from that answer.\n",
    "    use this format for generation :  start the generation with \"---\" and end it with \"---\" too ; between the questions you should include \"---\" as well . like this format bellow : \n",
    "    \n",
    "    ---\n",
    "    Question number 1\n",
    "    ---\n",
    "    Question number 2 \n",
    "    ---\n",
    "    Question number 3\n",
    "    --- \n",
    "    \n",
    "    Here is the answer : \n",
    "    Answer : {answer}\"\"\"\n",
    "    \n",
    "    prompt = prompt.format(answer=generated_answer)\n",
    "    \n",
    "    \n",
    "    generated_questions = llama.invoke(prompt).content\n",
    "    reason = copy.deepcopy(generated_questions)\n",
    "\n",
    "    \n",
    "    generated_questions = [item for item in generated_questions.split('---') if len(item) > 7]\n",
    "\n",
    "    #embed the question\n",
    "    generated_questions = [embeddings_model.embed_query(question) for question in generated_questions]\n",
    "    \n",
    "    #embed the query\n",
    "    original_query = embeddings_model.embed_query(original_query)\n",
    "    \n",
    "    generated_questions, original_query = np.array(generated_questions), np.array(original_query)\n",
    "    \n",
    "    # Normalize vectors\n",
    "    vec1_norm = original_query / np.linalg.norm(original_query)\n",
    "    vec_list_norm = generated_questions / np.linalg.norm(generated_questions, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = np.dot(vec_list_norm, vec1_norm)\n",
    "    \n",
    "    return reason, np.mean(cosine_sim)\n",
    "    "
   ],
   "id": "fbd35b7c67f42147",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T19:36:32.297291Z",
     "start_time": "2024-11-28T19:36:32.290420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def context_relevancy(retrieved_context, original_query):\n",
    "    \n",
    "    prompt = \"\"\"this is a context relevancy test. for the given context and question , extract each sentence of the context and determine if that sentence can potentially be helpful to answer the question. for every sentence , describe the relevancy of that sentence and answer in YES or NO terms which that sentence can be helpful to answer the question or not. \n",
    "    \n",
    "    use this format : \n",
    "    \n",
    "    Sentence  : a simple description of relevancy to the question : YES or NO\n",
    "    \n",
    "    Here is Question \n",
    "    Question : {query}\n",
    "    \n",
    "    Here is the Context :\n",
    "    Context : {context}\"\"\"\n",
    "    \n",
    "    prompt = prompt.format(query = original_query, context = retrieved_context)\n",
    "    \n",
    "    output = qwen.invoke(prompt).content\n",
    "    reason = copy.deepcopy(output)\n",
    "    \n",
    "    output = output.lower()\n",
    "    \n",
    "    score = output.count('yes') / (output.count('yes') + output.count('no'))\n",
    "    \n",
    "    return reason, score"
   ],
   "id": "ec587f2a38f15257",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T19:36:32.770453Z",
     "start_time": "2024-11-28T19:36:32.756448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pseudo_context_generate(query):\n",
    "    prompt = \"\"\"for the given question, generate a simple and small passage that can answer the question.\n",
    "    Here is the Question :\n",
    "    \n",
    "    Question : {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = prompt.format(question = query)\n",
    "    \n",
    "    output = llama.invoke(prompt).content\n",
    "\n",
    "    \n",
    "    return output"
   ],
   "id": "6cb3fec1285214cf",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T19:36:33.234587Z",
     "start_time": "2024-11-28T19:36:33.228059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def query_rewriting(query):\n",
    "    \n",
    "    prompt = f\"\"\"Please rewrite the query bellow for better retrieval in web search engines or retrieval augmented generation. just generate the rewrited query without any more explaination. generate only one rewrited query, only one.\n",
    "    \n",
    "    Here is the Query :\n",
    "    Query : {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = prompt.format(query = query)\n",
    "    \n",
    "    rewrited = llama.invoke(prompt).content\n",
    "\n",
    "    return rewrited"
   ],
   "id": "85147d9726611dfd",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading CRSB and SQUAD",
   "id": "994666f409da7cfc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:56:27.684717Z",
     "start_time": "2024-12-02T11:56:27.658558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json \n",
    "\n",
    "with open('F:\\\\OneDrive\\\\Desktop\\\\Research\\\\Dataset\\\\CRSB-Texts.json', 'r') as f:\n",
    "    crsb = json.load(f)\n",
    "    \n",
    "crsb = crsb['amazon_rainforest']"
   ],
   "id": "9d4667cfacb07c5d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:57:07.890696Z",
     "start_time": "2024-12-02T11:56:51.014489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "\n",
    "squad = datasets.load_dataset('rajpurkar/squad')\n",
    "squad = squad['validation'].shuffle()"
   ],
   "id": "33834b8a3fd4acf",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:57:09.625656Z",
     "start_time": "2024-12-02T11:57:09.600243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "squad = squad[:100]\n",
    "\n",
    "#this makes squad a dict like object with keys and values , values are lists"
   ],
   "id": "5257223831face75",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:57:10.204506Z",
     "start_time": "2024-12-02T11:57:10.189387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(crsb.keys())\n",
    "print(squad.keys())"
   ],
   "id": "cd97bcda60e083fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['contents', 'questions'])\n",
      "dict_keys(['id', 'title', 'context', 'question', 'answers'])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:57:10.716622Z",
     "start_time": "2024-12-02T11:57:10.706895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(crsb['contents']))\n",
    "print(len(squad['question']))"
   ],
   "id": "76e3730776c5f7b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:57:11.392553Z",
     "start_time": "2024-12-02T11:57:11.384349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "contexts = crsb['contents']\n",
    "questions = squad['question']"
   ],
   "id": "7e96a024611d6bf4",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG retriever",
   "id": "8bff0943acb3743a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T11:57:53.758572Z",
     "start_time": "2024-12-02T11:57:49.967672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "retriever = FAISS.from_texts(texts=contexts,\n",
    "                             embedding= embeddings_model)"
   ],
   "id": "c3fdeb3ade1ba53f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG Evaluation on CRSB + SQUAD",
   "id": "99ef35d676ce8882"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "\n",
    "crs = []\n",
    "ars = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    start = time()\n",
    "    retrieved_context = retriever.similarity_search(query=question, k =1)\n",
    "    _, ar = answer_relevancy(retrieved_context, question)\n",
    "    _, cr = context_relevancy(retrieved_context, question)\n",
    "    \n",
    "    crs.append(cr)\n",
    "    ars.append(ar)\n",
    "    \n",
    "    end = time()\n",
    "    print(f'Question {i} processed in {end - start} seconds')\n",
    "    print(f'CR score: {cr}, AR score: {ar}')\n"
   ],
   "id": "fe32620f99354600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ars, crs = np.array(ars), np.array(crs)\n",
    "\n",
    "print(f'ARs mean : {np.mean(ars)}')\n",
    "print(f'CRs mean : {np.mean(crs)}')"
   ],
   "id": "ed4b2c263865a732",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CAG Evaluation on CRSB + SQUAD",
   "id": "72430ee04fac7a5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from cag.vector_candidates.vc import VectorCandidates\n",
    "\n",
    "with open('F:\\\\OneDrive\\\\Desktop\\\\Research\\\\Dataset\\\\CRSB-Embeddings-MPNET.json', 'r') as f:\n",
    "    crsb = json.load(f)\n",
    "    \n",
    "crsb_contexts_embeddings = crsb['amazon_rainforest']['contents']\n",
    "crsb_pseudo_queries_embeddings = crsb['amazon_rainforest']['questions']"
   ],
   "id": "3f1e84d9687c7a8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VC = VectorCandidates(contexts= [ crsb_contexts_embeddings ], questions= [ crsb_pseudo_queries_embeddings ])",
   "id": "fa45e7191da2d90d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from cag.gate.vector_candidates import VectorCandidatesGate\n",
    "\n",
    "gate = VectorCandidatesGate(vc= VC, embedding_model= embeddings_model)"
   ],
   "id": "4b491aeb671a6d7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "ars = []\n",
    "crs = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    a = time.time()\n",
    "    \n",
    "    _, needs_retrieval = gate(squad['question'][i])\n",
    "    \n",
    "    if needs_retrieval:\n",
    "        retrieved_context = retriever.similarity_search(query=squad['question'][i], k =1)\n",
    "        _, ar = answer_relevancy(retrieved_context, squad['question'][i])\n",
    "        cr = context_relevancy(retrieved_context, squad['question'][i])\n",
    "    \n",
    "    else:\n",
    "        pseudo_context = pseudo_context_generate(squad['question'][i])\n",
    "        \n",
    "        _, ar = answer_relevancy(pseudo_context, squad['question'][i])\n",
    "        _, cr = context_relevancy(pseudo_context, squad['question'][i])\n",
    "        \n",
    "    ars.append(ar)\n",
    "    crs.append(cr)\n",
    "    b = time.time()\n",
    "    print(f'Question {i} processed in {b - a} seconds')\n",
    "    print(f'CR : {cr} --- AR : {ar}')"
   ],
   "id": "17cbd499420d2c21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ars, crs = np.array(ars), np.array(crs)\n",
    "\n",
    "print(f'ARs mean : {np.mean(ars)}')\n",
    "print(f'CRs mean : {np.mean(crs)}')"
   ],
   "id": "a0f32a2fa00e59d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HYDE Evaluation on CRSB + SQUAD",
   "id": "aee484f15a3cab1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "\n",
    "pseudos = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    #HyDE Step\n",
    "    pseudo_c = pseudo_context_generate(question)\n",
    "    pseudos.append(pseudo_c)\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    print(f'Pseudo-Context {i} processed in {end - start} seconds')\n",
    "    \n"
   ],
   "id": "ca0d8a3a1e36b38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "crs = []\n",
    "ars = []\n",
    "ar_reasons = []\n",
    "cr_reasons = []\n",
    "\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    retrieved_context = retriever.similarity_search(query=pseudos[i], k =1)\n",
    "    \n",
    "    ar_reason, ar = answer_relevancy(retrieved_context, question)\n",
    "    cr_reason, cr = context_relevancy(retrieved_context, question)\n",
    "    \n",
    "    crs.append(cr)\n",
    "    ars.append(ar)\n",
    "    ar_reasons.append(ar_reason)\n",
    "    cr_reasons.append(cr_reason)\n",
    "    \n",
    "    end = time()\n",
    "    print(f'Question {i} processed in {end - start} seconds')\n",
    "    print(f'CR score: {cr}, AR score: {ar}')\n"
   ],
   "id": "86025966a5e795ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ars, crs = np.array(ars), np.array(crs)\n",
    "\n",
    "print(f'ARs mean : {np.mean(ars)}')\n",
    "print(f'CRs mean : {np.mean(crs)}')"
   ],
   "id": "ddad084cbe25646e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Query Rewriting Evaluation on CRSB + SQUAD",
   "id": "27c58c8f1f763234"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "\n",
    "rewriteds = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    a = time()\n",
    "    # Query Rewriting Step\n",
    "    query_rewrited = query_rewriting(question)\n",
    "    rewriteds.append(query_rewrited)\n",
    "    \n",
    "    b = time()\n",
    "    \n",
    "    print(f'Rewriting {i} processed in {b - a} seconds')"
   ],
   "id": "aa9b68c112c6a347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crs = []\n",
    "ars = []\n",
    "ar_reasons = []\n",
    "cr_reasons = []\n",
    "\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    start = time()\n",
    "    retrieved_context = retriever.similarity_search(query=rewriteds[i] , k =1)\n",
    "    \n",
    "    ar_reason, ar = answer_relevancy(retrieved_context, question)\n",
    "    cr_reason, cr = context_relevancy(retrieved_context, question)\n",
    "    \n",
    "    crs.append(cr)\n",
    "    ars.append(ar)\n",
    "    ar_reasons.append(ar_reason)\n",
    "    cr_reasons.append(cr_reason)\n",
    "    \n",
    "    end = time()\n",
    "    print(f'Question {i} processed in {end - start} seconds')\n",
    "    print(f'CR score: {cr}, AR score: {ar}')\n"
   ],
   "id": "71148dc9249920cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ars, crs = np.array(ars), np.array(crs)\n",
    "\n",
    "print(f'ARs mean : {np.mean(ars)}')\n",
    "print(f'CRs mean : {np.mean(crs)}')"
   ],
   "id": "3082d5f32b18544c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluating Adpotive-RAG on CRSB and SQUAD",
   "id": "71a9abbfc87d4890"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T19:31:42.470182Z",
     "start_time": "2024-11-28T19:07:22.803866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "adapt_reasons = []\n",
    "classified = []\n",
    "times = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    a = time()\n",
    "    x, y = query_classification(contexts[i], question)\n",
    "    adapt_reasons.append(x)\n",
    "    classified.append(y)\n",
    "    b = time()\n",
    "    \n",
    "    times.append(b-a)\n",
    "    \n",
    "    print(f'Question {i} processed in {b - a} seconds')\n",
    "    \n",
    "print(np.mean(np.array(times)))"
   ],
   "id": "585ad58708b0c91a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0 processed in 17.21433663368225 seconds\n",
      "Question 1 processed in 11.587442398071289 seconds\n",
      "Question 2 processed in 16.3067626953125 seconds\n",
      "Question 3 processed in 17.906259536743164 seconds\n",
      "Question 4 processed in 14.323300838470459 seconds\n",
      "Question 5 processed in 8.344442129135132 seconds\n",
      "Question 6 processed in 10.027394533157349 seconds\n",
      "Question 7 processed in 13.33013367652893 seconds\n",
      "Question 8 processed in 9.292656183242798 seconds\n",
      "Question 9 processed in 9.36040472984314 seconds\n",
      "Question 10 processed in 12.670216083526611 seconds\n",
      "Question 11 processed in 12.168281555175781 seconds\n",
      "Question 12 processed in 12.082881927490234 seconds\n",
      "Question 13 processed in 13.635783195495605 seconds\n",
      "Question 14 processed in 11.90071702003479 seconds\n",
      "Question 15 processed in 18.116410970687866 seconds\n",
      "Question 16 processed in 14.134588718414307 seconds\n",
      "Question 17 processed in 15.46907925605774 seconds\n",
      "Question 18 processed in 13.189318895339966 seconds\n",
      "Question 19 processed in 21.319617986679077 seconds\n",
      "Question 20 processed in 15.271575689315796 seconds\n",
      "Question 21 processed in 13.435250759124756 seconds\n",
      "Question 22 processed in 18.263198137283325 seconds\n",
      "Question 23 processed in 12.776707172393799 seconds\n",
      "Question 24 processed in 21.842183589935303 seconds\n",
      "Question 25 processed in 10.815340995788574 seconds\n",
      "Question 26 processed in 13.274460315704346 seconds\n",
      "Question 27 processed in 14.744600057601929 seconds\n",
      "Question 28 processed in 16.338512897491455 seconds\n",
      "Question 29 processed in 13.1044921875 seconds\n",
      "Question 30 processed in 13.925787448883057 seconds\n",
      "Question 31 processed in 10.267146587371826 seconds\n",
      "Question 32 processed in 13.19065237045288 seconds\n",
      "Question 33 processed in 16.954296827316284 seconds\n",
      "Question 34 processed in 15.200256824493408 seconds\n",
      "Question 35 processed in 14.699206113815308 seconds\n",
      "Question 36 processed in 20.07736349105835 seconds\n",
      "Question 37 processed in 16.000610828399658 seconds\n",
      "Question 38 processed in 11.608221530914307 seconds\n",
      "Question 39 processed in 13.797415018081665 seconds\n",
      "Question 40 processed in 16.23664164543152 seconds\n",
      "Question 41 processed in 21.24166989326477 seconds\n",
      "Question 42 processed in 19.50668716430664 seconds\n",
      "Question 43 processed in 10.395072221755981 seconds\n",
      "Question 44 processed in 12.70869779586792 seconds\n",
      "Question 45 processed in 8.105399131774902 seconds\n",
      "Question 46 processed in 14.259458303451538 seconds\n",
      "Question 47 processed in 14.308028221130371 seconds\n",
      "Question 48 processed in 15.139876127243042 seconds\n",
      "Question 49 processed in 13.885978937149048 seconds\n",
      "Question 50 processed in 11.488098621368408 seconds\n",
      "Question 51 processed in 11.572152137756348 seconds\n",
      "Question 52 processed in 12.522678136825562 seconds\n",
      "Question 53 processed in 14.760932207107544 seconds\n",
      "Question 54 processed in 13.430508136749268 seconds\n",
      "Question 55 processed in 11.766931772232056 seconds\n",
      "Question 56 processed in 10.404902458190918 seconds\n",
      "Question 57 processed in 12.20631194114685 seconds\n",
      "Question 58 processed in 20.410138368606567 seconds\n",
      "Question 59 processed in 15.30501651763916 seconds\n",
      "Question 60 processed in 15.3997642993927 seconds\n",
      "Question 61 processed in 19.471766710281372 seconds\n",
      "Question 62 processed in 12.320878744125366 seconds\n",
      "Question 63 processed in 9.266399383544922 seconds\n",
      "Question 64 processed in 17.860737800598145 seconds\n",
      "Question 65 processed in 19.43234944343567 seconds\n",
      "Question 66 processed in 13.271069765090942 seconds\n",
      "Question 67 processed in 11.770289182662964 seconds\n",
      "Question 68 processed in 13.80777645111084 seconds\n",
      "Question 69 processed in 14.263310194015503 seconds\n",
      "Question 70 processed in 13.02037787437439 seconds\n",
      "Question 71 processed in 15.967784643173218 seconds\n",
      "Question 72 processed in 9.025514602661133 seconds\n",
      "Question 73 processed in 22.62754511833191 seconds\n",
      "Question 74 processed in 14.194390296936035 seconds\n",
      "Question 75 processed in 11.152958869934082 seconds\n",
      "Question 76 processed in 15.199893951416016 seconds\n",
      "Question 77 processed in 21.917534828186035 seconds\n",
      "Question 78 processed in 25.524959802627563 seconds\n",
      "Question 79 processed in 14.685834169387817 seconds\n",
      "Question 80 processed in 15.307651996612549 seconds\n",
      "Question 81 processed in 29.50376319885254 seconds\n",
      "Question 82 processed in 12.884510040283203 seconds\n",
      "Question 83 processed in 9.621636390686035 seconds\n",
      "Question 84 processed in 21.083983898162842 seconds\n",
      "Question 85 processed in 11.46034550666809 seconds\n",
      "Question 86 processed in 12.444996118545532 seconds\n",
      "Question 87 processed in 11.009742498397827 seconds\n",
      "Question 88 processed in 10.625611066818237 seconds\n",
      "Question 89 processed in 18.813059091567993 seconds\n",
      "Question 90 processed in 12.680588722229004 seconds\n",
      "Question 91 processed in 16.330260038375854 seconds\n",
      "Question 92 processed in 11.307043075561523 seconds\n",
      "Question 93 processed in 19.93190050125122 seconds\n",
      "Question 94 processed in 11.552666902542114 seconds\n",
      "Question 95 processed in 22.102771759033203 seconds\n",
      "Question 96 processed in 11.745809078216553 seconds\n",
      "Question 97 processed in 11.607771873474121 seconds\n",
      "Question 98 processed in 17.202536582946777 seconds\n",
      "Question 99 processed in 14.639554500579834 seconds\n",
      "14.596598265171052\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T20:39:11.453043Z",
     "start_time": "2024-11-28T19:36:49.100006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "ars = []\n",
    "crs = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "\n",
    "    a = time.time()\n",
    "    \n",
    "    needs_retrieval = classified[i]\n",
    "    \n",
    "    if needs_retrieval:\n",
    "        retrieved_context = retriever.similarity_search(question, k =1)\n",
    "        _, ar = answer_relevancy(retrieved_context, question)\n",
    "        cr = context_relevancy(retrieved_context, question)\n",
    "    \n",
    "    else:\n",
    "        pseudo_context = pseudo_context_generate(question)\n",
    "        \n",
    "        _, ar = answer_relevancy(pseudo_context, question)\n",
    "        _, cr = context_relevancy(pseudo_context, question)\n",
    "        \n",
    "    ars.append(ar)\n",
    "    crs.append(cr)\n",
    "    b = time.time()\n",
    "    print(f'Question {i} processed in {b - a} seconds')\n",
    "    print(f'CR : {cr} --- AR : {ar}')"
   ],
   "id": "35c4758d962befe0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0 processed in 28.522886514663696 seconds\n",
      "CR : 1.0 --- AR : 0.6100373989862063\n",
      "Question 1 processed in 72.37753963470459 seconds\n",
      "CR : 0.3333333333333333 --- AR : 0.49895905401186535\n",
      "Question 2 processed in 41.77283191680908 seconds\n",
      "CR : 0.5 --- AR : 0.758249211936661\n",
      "Question 3 processed in 89.19079422950745 seconds\n",
      "CR : 0.125 --- AR : 0.6986630425468255\n",
      "Question 4 processed in 66.12485122680664 seconds\n",
      "CR : 0.2 --- AR : 0.6486127845838747\n",
      "Question 5 processed in 67.31260299682617 seconds\n",
      "CR : 0.2 --- AR : 0.7017307862662019\n",
      "Question 6 processed in 90.14641618728638 seconds\n",
      "CR : 0.18181818181818182 --- AR : 0.5769554557977793\n",
      "Question 7 processed in 138.16933584213257 seconds\n",
      "CR : 0.5 --- AR : 0.7772152416862705\n",
      "Question 8 processed in 78.70425844192505 seconds\n",
      "CR : 0.36363636363636365 --- AR : 0.5699165148180174\n",
      "Question 9 processed in 90.88233733177185 seconds\n",
      "CR : 0.7777777777777778 --- AR : 0.6465251119158393\n",
      "Question 10 processed in 61.7341046333313 seconds\n",
      "CR : 0.25 --- AR : 0.6565391162115957\n",
      "Question 11 processed in 66.68988704681396 seconds\n",
      "CR : 0.5 --- AR : 0.5834331175303038\n",
      "Question 12 processed in 78.68351650238037 seconds\n",
      "CR : 1.0 --- AR : 0.7498598932343272\n",
      "Question 13 processed in 101.20226812362671 seconds\n",
      "CR : 0.16666666666666666 --- AR : 0.6009552650567987\n",
      "Question 14 processed in 82.16684865951538 seconds\n",
      "CR : 0.25 --- AR : 0.6919807762924145\n",
      "Question 15 processed in 75.75589084625244 seconds\n",
      "CR : 0.42857142857142855 --- AR : 0.6587146834113876\n",
      "Question 16 processed in 131.4290897846222 seconds\n",
      "CR : 0.45454545454545453 --- AR : 0.7603687855832373\n",
      "Question 17 processed in 91.8369414806366 seconds\n",
      "CR : 0.3333333333333333 --- AR : 0.6547897104674872\n",
      "Question 18 processed in 89.92089986801147 seconds\n",
      "CR : 0.2857142857142857 --- AR : 0.6480234570329059\n",
      "Question 19 processed in 96.27356362342834 seconds\n",
      "CR : 0.8571428571428571 --- AR : 0.6060771253281492\n",
      "Question 20 processed in 77.61940026283264 seconds\n",
      "CR : 0.125 --- AR : 0.5524600271751666\n",
      "Question 21 processed in 74.68422198295593 seconds\n",
      "CR : 0.2 --- AR : 0.5258967727581191\n",
      "Question 22 processed in 84.28232336044312 seconds\n",
      "CR : 0.14285714285714285 --- AR : 0.3948713578545906\n",
      "Question 23 processed in 71.16162395477295 seconds\n",
      "CR : 0.2 --- AR : 0.6313412944867453\n",
      "Question 24 processed in 89.29499292373657 seconds\n",
      "CR : 0.625 --- AR : 0.5782485307496598\n",
      "Question 25 processed in 100.8943772315979 seconds\n",
      "CR : 0.2 --- AR : 0.7283107111410508\n",
      "Question 26 processed in 45.7920184135437 seconds\n",
      "CR : 1.0 --- AR : 0.5444659603644549\n",
      "Question 27 processed in 87.38674211502075 seconds\n",
      "CR : 0.1111111111111111 --- AR : 0.5127495840912996\n",
      "Question 28 processed in 75.4629533290863 seconds\n",
      "CR : 0.2 --- AR : 0.5850424428014747\n",
      "Question 29 processed in 118.05389070510864 seconds\n",
      "CR : 0.5454545454545454 --- AR : 0.5772206393229495\n",
      "Question 30 processed in 128.5133879184723 seconds\n",
      "CR : 0.4 --- AR : 0.5655715121820794\n",
      "Question 31 processed in 85.8189799785614 seconds\n",
      "CR : 0.5 --- AR : 0.5163640471648903\n",
      "Question 32 processed in 96.34736442565918 seconds\n",
      "CR : 0.14285714285714285 --- AR : 0.5723788412092802\n",
      "Question 33 processed in 119.997567653656 seconds\n",
      "CR : 0.4666666666666667 --- AR : 0.4359272956530567\n",
      "Question 34 processed in 65.20182514190674 seconds\n",
      "CR : 0.5 --- AR : 0.7004689951742523\n",
      "Question 35 processed in 111.58533096313477 seconds\n",
      "CR : 0.125 --- AR : 0.5586190743897282\n",
      "Question 36 processed in 72.10673141479492 seconds\n",
      "CR : 0.2 --- AR : 0.541930485638119\n",
      "Question 37 processed in 104.80421209335327 seconds\n",
      "CR : 0.6 --- AR : 0.7175763371645952\n",
      "Question 38 processed in 99.73286938667297 seconds\n",
      "CR : 0.16666666666666666 --- AR : 0.6178896533725897\n",
      "Question 39 processed in 160.44192051887512 seconds\n",
      "CR : 0.4 --- AR : 0.5545806315003725\n",
      "Question 40 processed in 119.48124742507935 seconds\n",
      "CR : 0.375 --- AR : 0.6596953914990847\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[36], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m     pseudo_context \u001B[38;5;241m=\u001B[39m pseudo_context_generate(question)\n\u001B[0;32m     21\u001B[0m     _, ar \u001B[38;5;241m=\u001B[39m answer_relevancy(pseudo_context, question)\n\u001B[1;32m---> 22\u001B[0m     _, cr \u001B[38;5;241m=\u001B[39m \u001B[43mcontext_relevancy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpseudo_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m ars\u001B[38;5;241m.\u001B[39mappend(ar)\n\u001B[0;32m     25\u001B[0m crs\u001B[38;5;241m.\u001B[39mappend(cr)\n",
      "Cell \u001B[1;32mIn[26], line 17\u001B[0m, in \u001B[0;36mcontext_relevancy\u001B[1;34m(retrieved_context, original_query)\u001B[0m\n\u001B[0;32m      3\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mthis is a context relevancy test. for the given context and question , extract each sentence of the context and determine if that sentence can potentially be helpful to answer the question. for every sentence , describe the relevancy of that sentence and answer in YES or NO terms which that sentence can be helpful to answer the question or not. \u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124m\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124muse this format : \u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;124mHere is the Context :\u001B[39m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;124mContext : \u001B[39m\u001B[38;5;132;01m{context}\u001B[39;00m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m     15\u001B[0m prompt \u001B[38;5;241m=\u001B[39m prompt\u001B[38;5;241m.\u001B[39mformat(query \u001B[38;5;241m=\u001B[39m original_query, context \u001B[38;5;241m=\u001B[39m retrieved_context)\n\u001B[1;32m---> 17\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mqwen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcontent\n\u001B[0;32m     18\u001B[0m reason \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(output)\n\u001B[0;32m     20\u001B[0m output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mlower()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    276\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    277\u001B[0m     \u001B[38;5;28minput\u001B[39m: LanguageModelInput,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    282\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m    283\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    284\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[0;32m    285\u001B[0m         ChatGeneration,\n\u001B[1;32m--> 286\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    296\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    780\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[PromptValue],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    784\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m    785\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m--> 786\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n\u001B[0;32m    642\u001B[0m             run_managers[i]\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[1;32m--> 643\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[0;32m    644\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    645\u001B[0m     LLMResult(generations\u001B[38;5;241m=\u001B[39m[res\u001B[38;5;241m.\u001B[39mgenerations], llm_output\u001B[38;5;241m=\u001B[39mres\u001B[38;5;241m.\u001B[39mllm_output)  \u001B[38;5;66;03m# type: ignore[list-item]\u001B[39;00m\n\u001B[0;32m    646\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results\n\u001B[0;32m    647\u001B[0m ]\n\u001B[0;32m    648\u001B[0m llm_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_combine_llm_outputs([res\u001B[38;5;241m.\u001B[39mllm_output \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(messages):\n\u001B[0;32m    631\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 633\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    636\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    637\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    638\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    639\u001B[0m         )\n\u001B[0;32m    640\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    641\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    849\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    850\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 851\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    852\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    853\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    855\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_ollama\\chat_models.py:644\u001B[0m, in \u001B[0;36mChatOllama._generate\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    637\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate\u001B[39m(\n\u001B[0;32m    638\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    639\u001B[0m     messages: List[BaseMessage],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    642\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    643\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatResult:\n\u001B[1;32m--> 644\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chat_stream_with_aggregation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    645\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    647\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m final_chunk\u001B[38;5;241m.\u001B[39mgeneration_info\n\u001B[0;32m    648\u001B[0m     chat_generation \u001B[38;5;241m=\u001B[39m ChatGeneration(\n\u001B[0;32m    649\u001B[0m         message\u001B[38;5;241m=\u001B[39mAIMessage(\n\u001B[0;32m    650\u001B[0m             content\u001B[38;5;241m=\u001B[39mfinal_chunk\u001B[38;5;241m.\u001B[39mtext,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    654\u001B[0m         generation_info\u001B[38;5;241m=\u001B[39mgeneration_info,\n\u001B[0;32m    655\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_ollama\\chat_models.py:545\u001B[0m, in \u001B[0;36mChatOllama._chat_stream_with_aggregation\u001B[1;34m(self, messages, stop, run_manager, verbose, **kwargs)\u001B[0m\n\u001B[0;32m    536\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_chat_stream_with_aggregation\u001B[39m(\n\u001B[0;32m    537\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    538\u001B[0m     messages: List[BaseMessage],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    542\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    543\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatGenerationChunk:\n\u001B[0;32m    544\u001B[0m     final_chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_chat_stream\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mstream_resp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m            \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mChatGenerationChunk\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    548\u001B[0m \u001B[43m                \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mAIMessageChunk\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    562\u001B[0m \u001B[43m                \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    563\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_ollama\\chat_models.py:527\u001B[0m, in \u001B[0;36mChatOllama._create_chat_stream\u001B[1;34m(self, messages, stop, **kwargs)\u001B[0m\n\u001B[0;32m    517\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mchat(\n\u001B[0;32m    518\u001B[0m         model\u001B[38;5;241m=\u001B[39mparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    519\u001B[0m         messages\u001B[38;5;241m=\u001B[39mollama_messages,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    524\u001B[0m         tools\u001B[38;5;241m=\u001B[39mkwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtools\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    525\u001B[0m     )\n\u001B[0;32m    526\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 527\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client\u001B[38;5;241m.\u001B[39mchat(\n\u001B[0;32m    528\u001B[0m         model\u001B[38;5;241m=\u001B[39mparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    529\u001B[0m         messages\u001B[38;5;241m=\u001B[39mollama_messages,\n\u001B[0;32m    530\u001B[0m         stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    531\u001B[0m         options\u001B[38;5;241m=\u001B[39mOptions(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moptions\u001B[39m\u001B[38;5;124m\"\u001B[39m]),\n\u001B[0;32m    532\u001B[0m         keep_alive\u001B[38;5;241m=\u001B[39mparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkeep_alive\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    533\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mparams[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    534\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ollama\\_client.py:87\u001B[0m, in \u001B[0;36mClient._stream\u001B[1;34m(self, method, url, **kwargs)\u001B[0m\n\u001B[0;32m     84\u001B[0m   e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m     85\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m ResponseError(e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mtext, e\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mstatus_code) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m---> 87\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_lines\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m     88\u001B[0m \u001B[43m  \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[43m  \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m:=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpartial\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43merror\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_models.py:861\u001B[0m, in \u001B[0;36mResponse.iter_lines\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    859\u001B[0m decoder \u001B[38;5;241m=\u001B[39m LineDecoder()\n\u001B[0;32m    860\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[1;32m--> 861\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mline\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_models.py:848\u001B[0m, in \u001B[0;36mResponse.iter_text\u001B[1;34m(self, chunk_size)\u001B[0m\n\u001B[0;32m    846\u001B[0m chunker \u001B[38;5;241m=\u001B[39m TextChunker(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size)\n\u001B[0;32m    847\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[1;32m--> 848\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbyte_content\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_bytes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    849\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtext_content\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbyte_content\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    850\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_content\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_models.py:829\u001B[0m, in \u001B[0;36mResponse.iter_bytes\u001B[1;34m(self, chunk_size)\u001B[0m\n\u001B[0;32m    827\u001B[0m chunker \u001B[38;5;241m=\u001B[39m ByteChunker(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size)\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[1;32m--> 829\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraw_bytes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    830\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecoded\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_bytes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    831\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoded\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_models.py:883\u001B[0m, in \u001B[0;36mResponse.iter_raw\u001B[1;34m(self, chunk_size)\u001B[0m\n\u001B[0;32m    880\u001B[0m chunker \u001B[38;5;241m=\u001B[39m ByteChunker(chunk_size\u001B[38;5;241m=\u001B[39mchunk_size)\n\u001B[0;32m    882\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request):\n\u001B[1;32m--> 883\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mraw_stream_bytes\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    884\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_bytes_downloaded\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mraw_stream_bytes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    885\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_stream_bytes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_client.py:126\u001B[0m, in \u001B[0;36mBoundSyncStream.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    125\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m typing\u001B[38;5;241m.\u001B[39mIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[1;32m--> 126\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpx\\_transports\\default.py:113\u001B[0m, in \u001B[0;36mResponseStream.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m typing\u001B[38;5;241m.\u001B[39mIterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[0;32m    112\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[1;32m--> 113\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_httpcore_stream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:367\u001B[0m, in \u001B[0;36mPoolByteStream.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    366\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m--> 367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:363\u001B[0m, in \u001B[0;36mPoolByteStream.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    361\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__iter__\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[\u001B[38;5;28mbytes\u001B[39m]:\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 363\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stream\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    364\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpart\u001B[49m\n\u001B[0;32m    365\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:349\u001B[0m, in \u001B[0;36mHTTP11ConnectionByteStream.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    347\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ShieldCancellation():\n\u001B[0;32m    348\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m--> 349\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:341\u001B[0m, in \u001B[0;36mHTTP11ConnectionByteStream.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    339\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    340\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreceive_response_body\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request, kwargs):\n\u001B[1;32m--> 341\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_connection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_response_body\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m    342\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchunk\u001B[49m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[0;32m    344\u001B[0m     \u001B[38;5;66;03m# If we get an exception while streaming the response,\u001B[39;00m\n\u001B[0;32m    345\u001B[0m     \u001B[38;5;66;03m# we want to close the response (and possibly the connection)\u001B[39;00m\n\u001B[0;32m    346\u001B[0m     \u001B[38;5;66;03m# before raising that exception.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:210\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_response_body\u001B[1;34m(self, request)\u001B[0m\n\u001B[0;32m    207\u001B[0m timeout \u001B[38;5;241m=\u001B[39m timeouts\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m--> 210\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_receive_event\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(event, h11\u001B[38;5;241m.\u001B[39mData):\n\u001B[0;32m    212\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28mbytes\u001B[39m(event\u001B[38;5;241m.\u001B[39mdata)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_sync\\http11.py:224\u001B[0m, in \u001B[0;36mHTTP11Connection._receive_event\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    221\u001B[0m     event \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mnext_event()\n\u001B[0;32m    223\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m event \u001B[38;5;129;01mis\u001B[39;00m h11\u001B[38;5;241m.\u001B[39mNEED_DATA:\n\u001B[1;32m--> 224\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_network_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mREAD_NUM_BYTES\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    226\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    228\u001B[0m     \u001B[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001B[39;00m\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    230\u001B[0m     \u001B[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    234\u001B[0m     \u001B[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001B[39;00m\n\u001B[0;32m    235\u001B[0m     \u001B[38;5;66;03m# it as a ConnectError.\u001B[39;00m\n\u001B[0;32m    236\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;241m==\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_h11_state\u001B[38;5;241m.\u001B[39mtheir_state \u001B[38;5;241m==\u001B[39m h11\u001B[38;5;241m.\u001B[39mSEND_RESPONSE:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001B[0m, in \u001B[0;36mSyncStream.read\u001B[1;34m(self, max_bytes, timeout)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[0;32m    125\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sock\u001B[38;5;241m.\u001B[39msettimeout(timeout)\n\u001B[1;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-28T20:39:14.912822Z",
     "start_time": "2024-11-28T20:39:14.903630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ars, crs = np.array(ars), np.array(crs)\n",
    "\n",
    "print(f'ARs mean : {np.mean(ars)}')\n",
    "print(f'CRs mean : {np.mean(crs)}')"
   ],
   "id": "2462f2ffaf258de6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARs mean : 0.6138833199607734\n",
      "CRs mean : 0.3886134867842185\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d2ef86d807e1510b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
