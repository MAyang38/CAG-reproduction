{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from cag.embeddings import SentenceTransformerEmbeddings\n",
    "from cag.models import ChatOllama\n",
    "import copy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "embeddings_model = SentenceTransformerEmbeddings('sentence-transformers/all-mpnet-base-v2')",
   "id": "4cd25110fb018f0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "qwen = ChatOllama(model = 'qwen2.5', temprature = 0.001)",
   "id": "d0312c2443bcebdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "llama = ChatOllama(model = 'llama3.2', temprature = 0.001)",
   "id": "b266d7ec55751b33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def answer_relevancy(generated_answer, original_query):\n",
    "    \n",
    "    prompt = \"\"\"i will give you a answer , please generate three question which we can derive from that answer.\n",
    "    use this format for generation :  start the generation with \"---\" and end it with \"---\" too ; between the questions you should include \"---\" as well . like this format bellow : \n",
    "    \n",
    "    ---\n",
    "    Question number 1\n",
    "    ---\n",
    "    Question number 2 \n",
    "    ---\n",
    "    Question number 3\n",
    "    --- \n",
    "    \n",
    "    Here is the answer : \n",
    "    Answer : {answer}\"\"\"\n",
    "    \n",
    "    prompt = prompt.format(answer=generated_answer)\n",
    "    \n",
    "    \n",
    "    generated_questions = llama.invoke(prompt).content\n",
    "    reason = copy.deepcopy(generated_questions)\n",
    "\n",
    "    \n",
    "    generated_questions = [item for item in generated_questions.split('---') if len(item) > 7]\n",
    "\n",
    "    #embed the question\n",
    "    generated_questions = [embeddings_model.embed_query(question) for question in generated_questions]\n",
    "    \n",
    "    #embed the query\n",
    "    original_query = embeddings_model.embed_query(original_query)\n",
    "    \n",
    "    generated_questions, original_query = np.array(generated_questions), np.array(original_query)\n",
    "    \n",
    "    # Normalize vectors\n",
    "    vec1_norm = original_query / np.linalg.norm(original_query)\n",
    "    vec_list_norm = generated_questions / np.linalg.norm(generated_questions, axis=1, keepdims=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = np.dot(vec_list_norm, vec1_norm)\n",
    "    \n",
    "    return reason, np.mean(cosine_sim)\n",
    "    "
   ],
   "id": "fbd35b7c67f42147",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def context_relevancy(retrieved_context, original_query):\n",
    "    \n",
    "    prompt = \"\"\"this is a context relevancy test. for the given context and question , extract each sentence of the context and determine if that sentence can potentially be helpful to answer the question. for every sentence , describe the relevancy of that sentence and answer in YES or NO terms which that sentence can be helpful to answer the question or not. \n",
    "    \n",
    "    use this format : \n",
    "    \n",
    "    Sentence  : a simple description of relevancy to the question : YES or NO\n",
    "    \n",
    "    Here is Question \n",
    "    Question : {query}\n",
    "    \n",
    "    Here is the Context :\n",
    "    Context : {context}\"\"\"\n",
    "    \n",
    "    prompt = prompt.format(query = original_query, context = retrieved_context)\n",
    "    \n",
    "    output = qwen.invoke(prompt).content\n",
    "    reason = copy.deepcopy(output)\n",
    "    \n",
    "    output = output.lower()\n",
    "    \n",
    "    score = output.count('yes') / (output.count('yes') + output.count('no'))\n",
    "    \n",
    "    return reason, score"
   ],
   "id": "ec587f2a38f15257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def pseudo_context_generate(query):\n",
    "    prompt = \"\"\"for the given question, generate a simple and small passage that can answer the question.\n",
    "    Here is the Question :\n",
    "    \n",
    "    Question : {question}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = prompt.format(question = query)\n",
    "    \n",
    "    output = llama.invoke(prompt).content\n",
    "\n",
    "    \n",
    "    return output"
   ],
   "id": "6cb3fec1285214cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def query_rewriting(query):\n",
    "    \n",
    "    prompt = f\"\"\"Please rewrite the query bellow for better retrieval in web search engines or retrieval augmented generation. just generate the rewrited query without any more explaination. generate only one rewrited query, only one.\n",
    "    \n",
    "    Here is the Query :\n",
    "    Query : {query}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = prompt.format(query = query)\n",
    "    \n",
    "    rewrited = llama.invoke(prompt).content\n",
    "\n",
    "    return rewrited"
   ],
   "id": "85147d9726611dfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading CRSB and SQUAD",
   "id": "994666f409da7cfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json \n",
    "\n",
    "with open('F:\\\\OneDrive\\\\Desktop\\\\Research\\\\Dataset\\\\CRSB-Texts.json', 'r') as f:\n",
    "    crsb = json.load(f)\n",
    "    \n",
    "crsb = crsb['amazon_rainforest']"
   ],
   "id": "9d4667cfacb07c5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "\n",
    "squad = datasets.load_dataset('rajpurkar/squad')\n",
    "squad = squad['validation'].shuffle()"
   ],
   "id": "33834b8a3fd4acf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "squad = squad[:100]\n",
    "\n",
    "#this makes squad a dict like object with keys and values , values are lists"
   ],
   "id": "5257223831face75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(crsb.keys())\n",
    "print(squad.keys())"
   ],
   "id": "cd97bcda60e083fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(len(crsb['contents']))\n",
    "print(len(squad['question']))"
   ],
   "id": "76e3730776c5f7b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "contexts = crsb['contents']\n",
    "questions = squad['question']"
   ],
   "id": "7e96a024611d6bf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG retriever",
   "id": "8bff0943acb3743a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "retriever = FAISS.from_texts(texts=contexts,\n",
    "                             embedding= embeddings_model)"
   ],
   "id": "c3fdeb3ade1ba53f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# RAG Evaluation on CRSB + SQUAD",
   "id": "99ef35d676ce8882"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "\n",
    "crs = []\n",
    "ars = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    start = time()\n",
    "    retrieved_context = retriever.similarity_search(query=question, k =1)\n",
    "    _, ar = answer_relevancy(retrieved_context, question)\n",
    "    _, cr = context_relevancy(retrieved_context, question)\n",
    "    \n",
    "    crs.append(cr)\n",
    "    ars.append(ar)\n",
    "    \n",
    "    end = time()\n",
    "    print(f'Question {i} processed in {end - start} seconds')\n",
    "    print(f'CR score: {cr}, AR score: {ar}')\n"
   ],
   "id": "fe32620f99354600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ars, crs = np.array(ars), np.array(crs)\n",
    "\n",
    "print(f'ARs mean : {np.mean(ars)}')\n",
    "print(f'CRs mean : {np.mean(crs)}')"
   ],
   "id": "ed4b2c263865a732",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# CAG Evaluation on CRSB + SQUAD",
   "id": "72430ee04fac7a5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import json\n",
    "from cag.vector_candidates.vc import VectorCandidates\n",
    "\n",
    "with open('F:\\\\OneDrive\\\\Desktop\\\\Research\\\\Dataset\\\\CRSB-Embeddings-MPNET.json', 'r') as f:\n",
    "    crsb = json.load(f)\n",
    "    \n",
    "crsb_contexts_embeddings = crsb['amazon_rainforest']['contents']\n",
    "crsb_pseudo_queries_embeddings = crsb['amazon_rainforest']['questions']"
   ],
   "id": "3f1e84d9687c7a8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "VC = VectorCandidates(contexts= [ crsb_contexts_embeddings ], questions= [ crsb_pseudo_queries_embeddings ])",
   "id": "fa45e7191da2d90d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from cag.gate.vector_candidates import VectorCandidatesGate\n",
    "\n",
    "gate = VectorCandidatesGate(vc= VC, embedding_model= embeddings_model)"
   ],
   "id": "4b491aeb671a6d7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "ars = []\n",
    "crs = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    a = time.time()\n",
    "    \n",
    "    needs_retrieval = gate(squad['question'][i])\n",
    "    \n",
    "    if needs_retrieval:\n",
    "        retrieved_context = retriever.similarity_search(query=squad['question'][i], k =1)\n",
    "        _, ar = answer_relevancy(retrieved_context, squad['question'][i])\n",
    "        cr = context_relevancy(retrieved_context, squad['question'][i])\n",
    "    \n",
    "    else:\n",
    "        pseudo_context = pseudo_context_generate(squad['question'][i])\n",
    "        \n",
    "        _, ar = answer_relevancy(pseudo_context, squad['question'][i])\n",
    "        _, cr = context_relevancy(pseudo_context, squad['question'][i])\n",
    "        \n",
    "    ars.append(ar)\n",
    "    crs.append(cr)\n",
    "    b = time.time()\n",
    "    print(f'Question {i} processed in {b - a} seconds')\n",
    "    print(f'CR : {cr} --- AR : {ar}')"
   ],
   "id": "17cbd499420d2c21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ars, crs = np.array(ars), np.array(crs)\n",
    "\n",
    "print(f'ARs mean : {np.mean(ars)}')\n",
    "print(f'CRs mean : {np.mean(crs)}')"
   ],
   "id": "a0f32a2fa00e59d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# HYDE Evaluation on CRSB + SQUAD",
   "id": "aee484f15a3cab1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "\n",
    "pseudos = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    #HyDE Step\n",
    "    pseudo_c = pseudo_context_generate(question)\n",
    "    pseudos.append(pseudo_c)\n",
    "    \n",
    "    end = time()\n",
    "    \n",
    "    print(f'Pseudo-Context {i} processed in {end - start} seconds')\n",
    "    \n"
   ],
   "id": "ca0d8a3a1e36b38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "crs = []\n",
    "ars = []\n",
    "ar_reasons = []\n",
    "cr_reasons = []\n",
    "\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    retrieved_context = retriever.similarity_search(query=pseudos[i], k =1)\n",
    "    \n",
    "    ar_reason, ar = answer_relevancy(retrieved_context, question)\n",
    "    cr_reason, cr = context_relevancy(retrieved_context, question)\n",
    "    \n",
    "    crs.append(cr)\n",
    "    ars.append(ar)\n",
    "    ar_reasons.append(ar_reason)\n",
    "    cr_reasons.append(cr_reason)\n",
    "    \n",
    "    end = time()\n",
    "    print(f'Question {i} processed in {end - start} seconds')\n",
    "    print(f'CR score: {cr}, AR score: {ar}')\n"
   ],
   "id": "86025966a5e795ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ars, crs = np.array(ars), np.array(crs)\n",
    "\n",
    "print(f'ARs mean : {np.mean(ars)}')\n",
    "print(f'CRs mean : {np.mean(crs)}')"
   ],
   "id": "ddad084cbe25646e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Query Rewriting Evaluation on CRSB + SQUAD",
   "id": "27c58c8f1f763234"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from time import time\n",
    "\n",
    "rewriteds = []\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    a = time()\n",
    "    # Query Rewriting Step\n",
    "    query_rewrited = query_rewriting(question)\n",
    "    rewriteds.append(query_rewrited)\n",
    "    \n",
    "    b = time()\n",
    "    \n",
    "    print(f'Rewriting {i} processed in {b - a} seconds')"
   ],
   "id": "aa9b68c112c6a347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for item in rewriteds:\n",
    "    print(item, end = '\\n\\n')"
   ],
   "id": "163fe5defa780dfc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crs = []\n",
    "ars = []\n",
    "ar_reasons = []\n",
    "cr_reasons = []\n",
    "\n",
    "\n",
    "for i, question in enumerate(questions):\n",
    "    \n",
    "    start = time()\n",
    "    retrieved_context = retriever.similarity_search(query=rewriteds[i] , k =1)\n",
    "    \n",
    "    ar_reason, ar = answer_relevancy(retrieved_context, question)\n",
    "    cr_reason, cr = context_relevancy(retrieved_context, question)\n",
    "    \n",
    "    crs.append(cr)\n",
    "    ars.append(ar)\n",
    "    ar_reasons.append(ar_reason)\n",
    "    cr_reasons.append(cr_reason)\n",
    "    \n",
    "    end = time()\n",
    "    print(f'Question {i} processed in {end - start} seconds')\n",
    "    print(f'CR score: {cr}, AR score: {ar}')\n"
   ],
   "id": "71148dc9249920cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ars, crs = np.array(ars), np.array(crs)\n",
    "\n",
    "print(f'ARs mean : {np.mean(ars)}')\n",
    "print(f'CRs mean : {np.mean(crs)}')"
   ],
   "id": "3082d5f32b18544c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
